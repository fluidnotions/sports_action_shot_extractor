{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Action Shot Extractor - Learning Guide\n",
    "\n",
    "This notebook will guide you through understanding how the Action Shot Extractor works by exploring its components and running practical examples.\n",
    "\n",
    "## Overview\n",
    "\n",
    "Action Shot Extractor is an AI-powered tool that uses computer vision to identify and extract action shots from videos. It combines:\n",
    "- **YOLOv8** for object detection\n",
    "- **SegFormer** for image segmentation\n",
    "- **OpenCV** for video processing\n",
    "- **Color analysis** for filtering frames\n",
    "\n",
    "Let's explore each component step by step!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Environment Setup\n",
    "\n",
    "First, let's set up our environment and imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Add the project root to Python path\n",
    "project_root = os.path.abspath('../..')\n",
    "if project_root not in sys.path:\n",
    "    sys.path.insert(0, project_root)\n",
    "\n",
    "# Import Action Shot Extractor components\n",
    "from src.action_shot_extractor import (\n",
    "    FrameResult, RunSummary, run_pipeline,\n",
    "    hex_to_bgr, hue_distance\n",
    ")\n",
    "\n",
    "print(\"✅ Environment setup complete!\")\n",
    "print(f\"Project root: {project_root}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Understanding Color Analysis\n",
    "\n",
    "The core of Action Shot Extractor is color-based filtering. Let's explore how it works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Converting hex colors to BGR format (used by OpenCV)\n",
    "grass_green = \"#228B22\"\n",
    "sky_blue = \"#87CEEB\"\n",
    "jersey_red = \"#FF0000\"\n",
    "\n",
    "grass_bgr = hex_to_bgr(grass_green)\n",
    "sky_bgr = hex_to_bgr(sky_blue)\n",
    "jersey_bgr = hex_to_bgr(jersey_red)\n",
    "\n",
    "print(f\"Grass Green: {grass_green} → BGR: {grass_bgr}\")\n",
    "print(f\"Sky Blue: {sky_blue} → BGR: {sky_bgr}\")\n",
    "print(f\"Jersey Red: {jersey_red} → BGR: {jersey_bgr}\")\n",
    "\n",
    "# Calculate hue distances (for color matching)\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "def bgr_to_hue(bgr):\n",
    "    \"\"\"Convert BGR color to HSV hue value.\"\"\"\n",
    "    bgr_array = np.uint8([[bgr]])\n",
    "    hsv = cv2.cvtColor(bgr_array, cv2.COLOR_BGR2HSV)\n",
    "    return hsv[0][0][0]\n",
    "\n",
    "grass_hue = bgr_to_hue(grass_bgr)\n",
    "sky_hue = bgr_to_hue(sky_bgr)\n",
    "\n",
    "distance = hue_distance(grass_hue, sky_hue)\n",
    "print(f\"\\nHue distance between grass and sky: {distance}°\")\n",
    "print(f\"Grass hue: {grass_hue}°, Sky hue: {sky_hue}°\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Video Processing Basics\n",
    "\n",
    "Let's understand how video frames are processed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create a sample \"frame\" to demonstrate processing\n",
    "def create_sample_frame():\n",
    "    \"\"\"Create a synthetic sports-like frame for demonstration.\"\"\"\n",
    "    frame = np.zeros((480, 640, 3), dtype=np.uint8)\n",
    "    \n",
    "    # Green field background\n",
    "    frame[:, :] = [34, 139, 34]  # Forest green in BGR\n",
    "    \n",
    "    # Add some \"players\" (colored rectangles)\n",
    "    cv2.rectangle(frame, (100, 200), (150, 300), (0, 0, 255), -1)  # Red player\n",
    "    cv2.rectangle(frame, (300, 180), (350, 280), (255, 255, 255), -1)  # White player\n",
    "    cv2.rectangle(frame, (500, 220), (550, 320), (0, 255, 255), -1)  # Yellow player\n",
    "    \n",
    "    # Add a \"ball\" (white circle)\n",
    "    cv2.circle(frame, (320, 240), 15, (255, 255, 255), -1)\n",
    "    \n",
    "    return frame\n",
    "\n",
    "sample_frame = create_sample_frame()\n",
    "\n",
    "# Display the sample frame\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.imshow(cv2.cvtColor(sample_frame, cv2.COLOR_BGR2RGB))\n",
    "plt.title('Sample Sports Frame')\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "print(f\"Frame shape: {sample_frame.shape}\")\n",
    "print(f\"Frame dtype: {sample_frame.dtype}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Color-Based Frame Analysis\n",
    "\n",
    "Let's analyze the color composition of our sample frame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_frame_colors(frame, target_color_bgr, tolerance=30):\n",
    "    \"\"\"Analyze how much of a frame contains a target color.\"\"\"\n",
    "    # Convert to HSV for better color matching\n",
    "    hsv_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2HSV)\n",
    "    target_hsv = cv2.cvtColor(np.uint8([[target_color_bgr]]), cv2.COLOR_BGR2HSV)[0][0]\n",
    "    \n",
    "    # Create color mask\n",
    "    lower = np.array([max(0, target_hsv[0] - tolerance), 50, 50])\n",
    "    upper = np.array([min(179, target_hsv[0] + tolerance), 255, 255])\n",
    "    mask = cv2.inRange(hsv_frame, lower, upper)\n",
    "    \n",
    "    # Calculate percentage\n",
    "    total_pixels = frame.shape[0] * frame.shape[1]\n",
    "    matching_pixels = cv2.countNonZero(mask)\n",
    "    percentage = (matching_pixels / total_pixels) * 100\n",
    "    \n",
    "    return percentage, mask\n",
    "\n",
    "# Analyze green (field) content\n",
    "green_percentage, green_mask = analyze_frame_colors(sample_frame, [34, 139, 34])\n",
    "\n",
    "# Analyze red (player) content\n",
    "red_percentage, red_mask = analyze_frame_colors(sample_frame, [0, 0, 255])\n",
    "\n",
    "print(f\"Green (field) coverage: {green_percentage:.1f}%\")\n",
    "print(f\"Red (player) coverage: {red_percentage:.1f}%\")\n",
    "\n",
    "# Visualize masks\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "axes[0].imshow(cv2.cvtColor(sample_frame, cv2.COLOR_BGR2RGB))\n",
    "axes[0].set_title('Original Frame')\n",
    "axes[0].axis('off')\n",
    "\n",
    "axes[1].imshow(green_mask, cmap='gray')\n",
    "axes[1].set_title(f'Green Mask ({green_percentage:.1f}%)')\n",
    "axes[1].axis('off')\n",
    "\n",
    "axes[2].imshow(red_mask, cmap='gray')\n",
    "axes[2].set_title(f'Red Mask ({red_percentage:.1f}%)')\n",
    "axes[2].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Understanding the Pipeline Components\n",
    "\n",
    "Let's explore the main components of the Action Shot Extractor pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read and understand the pipeline structure\n",
    "from src.action_shot_extractor.pipeline import FrameResult, RunSummary\n",
    "\n",
    "# Create a sample FrameResult\n",
    "sample_result = FrameResult(\n",
    "    frame_number=150,\n",
    "    timestamp=5.0,  # 5 seconds into video\n",
    "    sharpness_score=0.85,\n",
    "    color_match_score=0.92,\n",
    "    objects_detected=['person', 'sports ball'],\n",
    "    confidence_scores=[0.89, 0.76],\n",
    "    selected=True,\n",
    "    reason=\"High action content with clear objects\"\n",
    ")\n",
    "\n",
    "print(\"Sample FrameResult:\")\n",
    "print(f\"  Frame: {sample_result.frame_number} at {sample_result.timestamp}s\")\n",
    "print(f\"  Sharpness: {sample_result.sharpness_score:.2f}\")\n",
    "print(f\"  Color match: {sample_result.color_match_score:.2f}\")\n",
    "print(f\"  Objects: {sample_result.objects_detected}\")\n",
    "print(f\"  Confidences: {sample_result.confidence_scores}\")\n",
    "print(f\"  Selected: {sample_result.selected}\")\n",
    "print(f\"  Reason: {sample_result.reason}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Sharpness Detection\n",
    "\n",
    "Sharpness is crucial for identifying clear action shots:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_sharpness(frame):\n",
    "    \"\"\"Calculate frame sharpness using Laplacian variance.\"\"\"\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    laplacian = cv2.Laplacian(gray, cv2.CV_64F)\n",
    "    variance = laplacian.var()\n",
    "    return variance\n",
    "\n",
    "def create_blurred_frame(frame, blur_amount):\n",
    "    \"\"\"Create a blurred version of the frame.\"\"\"\n",
    "    return cv2.GaussianBlur(frame, (blur_amount, blur_amount), 0)\n",
    "\n",
    "# Test sharpness on different blur levels\n",
    "blur_levels = [1, 15, 31, 51]\n",
    "sharpness_scores = []\n",
    "\n",
    "fig, axes = plt.subplots(2, len(blur_levels), figsize=(16, 8))\n",
    "\n",
    "for i, blur in enumerate(blur_levels):\n",
    "    if blur == 1:\n",
    "        test_frame = sample_frame.copy()\n",
    "        title = \"Original\"\n",
    "    else:\n",
    "        test_frame = create_blurred_frame(sample_frame, blur)\n",
    "        title = f\"Blur {blur}px\"\n",
    "    \n",
    "    sharpness = calculate_sharpness(test_frame)\n",
    "    sharpness_scores.append(sharpness)\n",
    "    \n",
    "    # Display frame\n",
    "    axes[0, i].imshow(cv2.cvtColor(test_frame, cv2.COLOR_BGR2RGB))\n",
    "    axes[0, i].set_title(f\"{title}\\nSharpness: {sharpness:.0f}\")\n",
    "    axes[0, i].axis('off')\n",
    "    \n",
    "    # Display edge detection\n",
    "    gray = cv2.cvtColor(test_frame, cv2.COLOR_BGR2GRAY)\n",
    "    edges = cv2.Laplacian(gray, cv2.CV_64F)\n",
    "    axes[1, i].imshow(np.abs(edges), cmap='gray')\n",
    "    axes[1, i].set_title('Edge Detection')\n",
    "    axes[1, i].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Plot sharpness trend\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(blur_levels, sharpness_scores, 'bo-', linewidth=2, markersize=8)\n",
    "plt.xlabel('Blur Level (pixels)')\n",
    "plt.ylabel('Sharpness Score')\n",
    "plt.title('Sharpness vs Blur Level')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(\"Observation: Sharpness score decreases as blur increases\")\n",
    "print(\"This helps identify clear, action-worthy frames!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Understanding the CLI Interface\n",
    "\n",
    "Let's explore the command-line interface structure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the CLI module to understand available options\n",
    "cli_path = os.path.join(project_root, 'src', 'action_shot_extractor', 'cli.py')\n",
    "\n",
    "print(\"CLI Command Structure:\")\n",
    "print(\"action-shot-extractor [OPTIONS] VIDEO_PATH\")\n",
    "print()\n",
    "print(\"Key Options:\")\n",
    "print(\"  --colors TEXT        Target colors (hex format, comma-separated)\")\n",
    "print(\"  --output-dir PATH    Output directory for extracted frames\")\n",
    "print(\"  --max-frames INT     Maximum frames to extract\")\n",
    "print(\"  --min-sharpness FLOAT Minimum sharpness threshold\")\n",
    "print(\"  --color-tolerance INT Color matching tolerance\")\n",
    "print(\"  --fps-sample INT     Sample every N frames for performance\")\n",
    "print()\n",
    "print(\"Example usage:\")\n",
    "print('action-shot-extractor --colors \"#00FF00,#FF0000\" --max-frames 10 video.mp4')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Putting It All Together\n",
    "\n",
    "Let's simulate a complete pipeline run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_pipeline_run():\n",
    "    \"\"\"Simulate a complete Action Shot Extractor pipeline run.\"\"\"\n",
    "    \n",
    "    # Simulate processing multiple frames\n",
    "    frames_data = [\n",
    "        (50, 1.67, 0.65, 0.45, ['person'], [0.75], False, \"Low sharpness\"),\n",
    "        (150, 5.0, 0.85, 0.92, ['person', 'sports ball'], [0.89, 0.76], True, \"High action content\"),\n",
    "        (300, 10.0, 0.55, 0.88, ['person'], [0.82], False, \"Motion blur detected\"),\n",
    "        (450, 15.0, 0.91, 0.87, ['person', 'person'], [0.91, 0.85], True, \"Multiple players detected\"),\n",
    "        (600, 20.0, 0.78, 0.23, ['person'], [0.66], False, \"Poor color match\"),\n",
    "    ]\n",
    "    \n",
    "    results = []\n",
    "    for data in frames_data:\n",
    "        result = FrameResult(\n",
    "            frame_number=data[0],\n",
    "            timestamp=data[1],\n",
    "            sharpness_score=data[2],\n",
    "            color_match_score=data[3],\n",
    "            objects_detected=data[4],\n",
    "            confidence_scores=data[5],\n",
    "            selected=data[6],\n",
    "            reason=data[7]\n",
    "        )\n",
    "        results.append(result)\n",
    "    \n",
    "    # Create summary\n",
    "    selected_frames = [r for r in results if r.selected]\n",
    "    \n",
    "    summary = RunSummary(\n",
    "        total_frames=len(results),\n",
    "        processed_frames=len(results),\n",
    "        selected_frames=len(selected_frames),\n",
    "        avg_sharpness=sum(r.sharpness_score for r in results) / len(results),\n",
    "        avg_color_match=sum(r.color_match_score for r in results) / len(results),\n",
    "        processing_time=12.5,\n",
    "        output_directory=\"./output/\"\n",
    "    )\n",
    "    \n",
    "    return results, summary\n",
    "\n",
    "# Run simulation\n",
    "results, summary = simulate_pipeline_run()\n",
    "\n",
    "print(\"🎬 Pipeline Simulation Results\")\n",
    "print(\"=\" * 40)\n",
    "print(f\"Total frames processed: {summary.total_frames}\")\n",
    "print(f\"Frames selected: {summary.selected_frames}\")\n",
    "print(f\"Selection rate: {(summary.selected_frames/summary.total_frames)*100:.1f}%\")\n",
    "print(f\"Average sharpness: {summary.avg_sharpness:.2f}\")\n",
    "print(f\"Average color match: {summary.avg_color_match:.2f}\")\n",
    "print(f\"Processing time: {summary.processing_time:.1f}s\")\n",
    "print()\n",
    "\n",
    "print(\"Selected Frames:\")\n",
    "for result in results:\n",
    "    if result.selected:\n",
    "        print(f\"  Frame {result.frame_number} ({result.timestamp}s): {result.reason}\")\n",
    "\n",
    "# Visualize results\n",
    "frame_numbers = [r.frame_number for r in results]\n",
    "sharpness_scores = [r.sharpness_score for r in results]\n",
    "color_scores = [r.color_match_score for r in results]\n",
    "selected = [r.selected for r in results]\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Plot sharpness and color scores\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(frame_numbers, sharpness_scores, 'b-o', label='Sharpness', alpha=0.7)\n",
    "plt.plot(frame_numbers, color_scores, 'g-s', label='Color Match', alpha=0.7)\n",
    "\n",
    "# Highlight selected frames\n",
    "selected_frames = [frame_numbers[i] for i, sel in enumerate(selected) if sel]\n",
    "selected_sharpness = [sharpness_scores[i] for i, sel in enumerate(selected) if sel]\n",
    "plt.scatter(selected_frames, selected_sharpness, c='red', s=100, marker='*', \n",
    "           label='Selected Frames', zorder=5)\n",
    "\n",
    "plt.xlabel('Frame Number')\n",
    "plt.ylabel('Score')\n",
    "plt.title('Frame Analysis Results')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Show selection timeline\n",
    "plt.subplot(2, 1, 2)\n",
    "selection_y = [1 if sel else 0 for sel in selected]\n",
    "plt.scatter(frame_numbers, selection_y, c=['red' if sel else 'gray' for sel in selected], \n",
    "           s=[100 if sel else 50 for sel in selected], alpha=0.7)\n",
    "plt.xlabel('Frame Number')\n",
    "plt.ylabel('Selected')\n",
    "plt.title('Frame Selection Timeline')\n",
    "plt.yticks([0, 1], ['No', 'Yes'])\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Next Steps and Exercises\n",
    "\n",
    "Now that you understand the components, try these exercises:\n",
    "\n",
    "### Exercise 1: Color Experimentation\n",
    "Modify the color analysis function to work with different sports:\n",
    "- Basketball (orange ball, wooden court)\n",
    "- Tennis (green court, yellow ball)\n",
    "- Soccer (green field, white ball)\n",
    "\n",
    "### Exercise 2: Custom Sharpness Metrics\n",
    "Implement alternative sharpness detection methods:\n",
    "- Sobel edge detection\n",
    "- Variance of gradients\n",
    "- FFT-based methods\n",
    "\n",
    "### Exercise 3: Pipeline Optimization\n",
    "Think about how to optimize the pipeline:\n",
    "- Skip frames vs. process all frames\n",
    "- Multi-threading for faster processing\n",
    "- GPU acceleration for AI models\n",
    "\n",
    "### Exercise 4: Real Video Testing\n",
    "Test the tool with actual video files:\n",
    "```bash\n",
    "# Install and test\n",
    "cd ../.. \n",
    "poetry install\n",
    "action-shot-extractor --help\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "You've now learned:\n",
    "1. ✅ How color analysis works in sports video processing\n",
    "2. ✅ How sharpness detection identifies clear frames\n",
    "3. ✅ How the pipeline combines multiple metrics for frame selection\n",
    "4. ✅ How to interpret and visualize processing results\n",
    "\n",
    "The Action Shot Extractor combines computer vision, AI object detection, and traditional image processing to automatically find the best moments in sports videos. Each component serves a specific purpose in creating a robust action shot detection system.\n",
    "\n",
    "**Happy learning! 🚀**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}