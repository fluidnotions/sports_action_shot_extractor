{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Template Matching and Feature Detection Exercises\n",
    "\n",
    "This notebook extends the Action Shot Extractor learning guide with hands-on exercises in template matching and feature detection - the core techniques behind player identification.\n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "1. **Template Matching** - How to find objects using reference images\n",
    "2. **ORB Feature Detection** - Advanced feature-based matching\n",
    "3. **Multi-Angle Matching** - Handling different player orientations\n",
    "4. **Performance Comparison** - When to use each method\n",
    "\n",
    "Let's dive in!"
   ]
  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"## 1. Environment Setup\\n\",\n    \"\\n\",\n    \"First, let's import everything we need:\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"import cv2\\n\",\n    \"import numpy as np\\n\",\n    \"import matplotlib.pyplot as plt\\n\",\n    \"from pathlib import Path\\n\",\n    \"import sys\\n\",\n    \"import os\\n\",\n    \"\\n\",\n    \"# Add project root to path\\n\",\n    \"project_root = os.path.abspath('../..')\\n\",\n    \"if project_root not in sys.path:\\n\",\n    \"    sys.path.insert(0, project_root)\\n\",\n    \"\\n\",\n    \"# Import our new player matcher\\n\",\n    \"from src.action_shot_extractor.player_matcher import PlayerMatcher, create_player_matcher\\n\",\n    \"\\n\",\n    \"print(\\\"‚úÖ Environment ready for template matching and feature detection!\\\")\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"## 2. Creating Synthetic Sports Scenes\\n\",\n    \"\\n\",\n    \"Let's create more realistic test scenes with multiple players:\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"def create_soccer_scene(player_positions, target_player_id=0, ball_pos=None):\\n\",\n    \"    \\\"\\\"\\\"Create a synthetic soccer scene with multiple players.\\\"\\\"\\\"\\n\",\n    \"    # Create field background\\n\",\n    \"    scene = np.zeros((600, 800, 3), dtype=np.uint8)\\n\",\n    \"    scene[:, :] = [34, 139, 34]  # Green field\\n\",\n    \"    \\n\",\n    \"    # Add field markings\\n\",\n    \"    cv2.line(scene, (400, 0), (400, 600), (255, 255, 255), 3)  # Center line\\n\",\n    \"    cv2.circle(scene, (400, 300), 100, (255, 255, 255), 3)  # Center circle\\n\",\n    \"    \\n\",\n    \"    # Player colors and shapes\\n\",\n    \"    player_colors = [\\n\",\n    \"        (0, 0, 255),    # Red (target player)\\n\",\n    \"        (255, 0, 0),    # Blue (team 1)\\n\",\n    \"        (255, 0, 0),    # Blue (team 1)\\n\",\n    \"        (0, 255, 255),  # Yellow (team 2)\\n\",\n    \"        (0, 255, 255),  # Yellow (team 2)\\n\",\n    \"        (255, 255, 255) # White (goalkeeper)\\n\",\n    \"    ]\\n\",\n    \"    \\n\",\n    \"    # Draw players\\n\",\n    \"    for i, (x, y) in enumerate(player_positions):\\n\",\n    \"        color = player_colors[i % len(player_colors)]\\n\",\n    \"        \\n\",\n    \"        # Make target player slightly larger\\n\",\n    \"        size = 35 if i == target_player_id else 30\\n\",\n    \"        \\n\",\n    \"        # Draw player as circle (overhead view)\\n\",\n    \"        cv2.circle(scene, (x, y), size, color, -1)\\n\",\n    \"        \\n\",\n    \"        # Add player number\\n\",\n    \"        cv2.putText(scene, str(i+1), (x-8, y+5), \\n\",\n    \"                   cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 0, 0), 2)\\n\",\n    \"    \\n\",\n    \"    # Add ball if specified\\n\",\n    \"    if ball_pos:\\n\",\n    \"        cv2.circle(scene, ball_pos, 8, (255, 255, 255), -1)\\n\",\n    \"        cv2.circle(scene, ball_pos, 8, (0, 0, 0), 2)\\n\",\n    \"    \\n\",\n    \"    return scene\\n\",\n    \"\\n\",\n    \"def create_reference_player(color, size=50, view='front'):\\n\",\n    \"    \\\"\\\"\\\"Create a reference image of a player from different angles.\\\"\\\"\\\"\\n\",\n    \"    ref_img = np.zeros((100, 100, 3), dtype=np.uint8)\\n\",\n    \"    ref_img[:, :] = [34, 139, 34]  # Green background\\n\",\n    \"    \\n\",\n    \"    center = (50, 50)\\n\",\n    \"    \\n\",\n    \"    if view == 'front':\\n\",\n    \"        # Front view - circular\\n\",\n    \"        cv2.circle(ref_img, center, size//2, color, -1)\\n\",\n    \"        # Add face details\\n\",\n    \"        cv2.circle(ref_img, (45, 45), 3, (0, 0, 0), -1)  # Eye\\n\",\n    \"        cv2.circle(ref_img, (55, 45), 3, (0, 0, 0), -1)  # Eye\\n\",\n    \"    elif view == 'side':\\n\",\n    \"        # Side view - oval\\n\",\n    \"        cv2.ellipse(ref_img, center, (size//3, size//2), 0, 0, 360, color, -1)\\n\",\n    \"        # Add side details\\n\",\n    \"        cv2.circle(ref_img, (55, 45), 3, (0, 0, 0), -1)  # Eye\\n\",\n    \"    elif view == 'back':\\n\",\n    \"        # Back view - circular with different details\\n\",\n    \"        cv2.circle(ref_img, center, size//2, color, -1)\\n\",\n    \"        # Add back details (number)\\n\",\n    \"        cv2.putText(ref_img, '1', (45, 55), \\n\",\n    \"                   cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 0), 2)\\n\",\n    \"    \\n\",\n    \"    return ref_img\\n\",\n    \"\\n\",\n    \"# Create test scenes\\n\",\n    \"scene1_positions = [(150, 200), (300, 180), (450, 220), (600, 200), (500, 400), (100, 500)]\\n\",\n    \"scene1 = create_soccer_scene(scene1_positions, target_player_id=0, ball_pos=(180, 220))\\n\",\n    \"\\n\",\n    \"scene2_positions = [(200, 300), (350, 250), (500, 280), (300, 450), (150, 350), (650, 300)]\\n\",\n    \"scene2 = create_soccer_scene(scene2_positions, target_player_id=0, ball_pos=(230, 320))\\n\",\n    \"\\n\",\n    \"# Create reference images for red player\\n\",\n    \"ref_front = create_reference_player((0, 0, 255), view='front')\\n\",\n    \"ref_side = create_reference_player((0, 0, 255), view='side')\\n\",\n    \"ref_back = create_reference_player((0, 0, 255), view='back')\\n\",\n    \"\\n\",\n    \"# Visualize\\n\",\n    \"fig, axes = plt.subplots(2, 3, figsize=(15, 10))\\n\",\n    \"\\n\",\n    \"# Show scenes\\n\",\n    \"axes[0, 0].imshow(cv2.cvtColor(scene1, cv2.COLOR_BGR2RGB))\\n\",\n    \"axes[0, 0].set_title('Soccer Scene 1\\\\n(Ball near red player)')\\n\",\n    \"axes[0, 0].axis('off')\\n\",\n    \"\\n\",\n    \"axes[0, 1].imshow(cv2.cvtColor(scene2, cv2.COLOR_BGR2RGB))\\n\",\n    \"axes[0, 1].set_title('Soccer Scene 2\\\\n(Different positions)')\\n\",\n    \"axes[0, 1].axis('off')\\n\",\n    \"\\n\",\n    \"axes[0, 2].axis('off')\\n\",\n    \"\\n\",\n    \"# Show reference images\\n\",\n    \"axes[1, 0].imshow(cv2.cvtColor(ref_front, cv2.COLOR_BGR2RGB))\\n\",\n    \"axes[1, 0].set_title('Reference: Front View')\\n\",\n    \"axes[1, 0].axis('off')\\n\",\n    \"\\n\",\n    \"axes[1, 1].imshow(cv2.cvtColor(ref_side, cv2.COLOR_BGR2RGB))\\n\",\n    \"axes[1, 1].set_title('Reference: Side View')\\n\",\n    \"axes[1, 1].axis('off')\\n\",\n    \"\\n\",\n    \"axes[1, 2].imshow(cv2.cvtColor(ref_back, cv2.COLOR_BGR2RGB))\\n\",\n    \"axes[1, 2].set_title('Reference: Back View')\\n\",\n    \"axes[1, 2].axis('off')\\n\",\n    \"\\n\",\n    \"plt.tight_layout()\\n\",\n    \"plt.show()\\n\",\n    \"\\n\",\n    \"print(\\\"‚úÖ Test scenes and reference images created!\\\")\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"## 3. Exercise 1: Template Matching Basics\\n\",\n    \"\\n\",\n    \"Let's start with simple template matching:\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"def template_match_demo(scene, template, method=cv2.TM_CCOEFF_NORMED):\\n\",\n    \"    \\\"\\\"\\\"Demonstrate template matching with visualization.\\\"\\\"\\\"\\n\",\n    \"    \\n\",\n    \"    # Perform template matching\\n\",\n    \"    result = cv2.matchTemplate(scene, template, method)\\n\",\n    \"    \\n\",\n    \"    # Find the best match\\n\",\n    \"    min_val, max_val, min_loc, max_loc = cv2.minMaxLoc(result)\\n\",\n    \"    \\n\",\n    \"    # Choose location based on method\\n\",\n    \"    if method in [cv2.TM_SQDIFF, cv2.TM_SQDIFF_NORMED]:\\n\",\n    \"        top_left = min_loc\\n\",\n    \"        confidence = 1 - min_val\\n\",\n    \"    else:\\n\",\n    \"        top_left = max_loc\\n\",\n    \"        confidence = max_val\\n\",\n    \"    \\n\",\n    \"    # Get template dimensions\\n\",\n    \"    h, w = template.shape[:2]\\n\",\n    \"    bottom_right = (top_left[0] + w, top_left[1] + h)\\n\",\n    \"    \\n\",\n    \"    # Draw bounding box\\n\",\n    \"    result_img = scene.copy()\\n\",\n    \"    cv2.rectangle(result_img, top_left, bottom_right, (0, 255, 0), 3)\\n\",\n    \"    \\n\",\n    \"    # Add confidence text\\n\",\n    \"    cv2.putText(result_img, f'Conf: {confidence:.3f}', \\n\",\n    \"               (top_left[0], top_left[1] - 10),\\n\",\n    \"               cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)\\n\",\n    \"    \\n\",\n    \"    return result_img, result, confidence, (top_left, bottom_right)\\n\",\n    \"\\n\",\n    \"# Test template matching with different methods\\n\",\n    \"methods = [\\n\",\n    \"    ('TM_CCOEFF_NORMED', cv2.TM_CCOEFF_NORMED),\\n\",\n    \"    ('TM_CCORR_NORMED', cv2.TM_CCORR_NORMED),\\n\",\n    \"    ('TM_SQDIFF_NORMED', cv2.TM_SQDIFF_NORMED)\\n\",\n    \"]\\n\",\n    \"\\n\",\n    \"fig, axes = plt.subplots(2, 3, figsize=(18, 12))\\n\",\n    \"\\n\",\n    \"for i, (name, method) in enumerate(methods):\\n\",\n    \"    # Match front view reference against scene 1\\n\",\n    \"    result_img, match_result, confidence, bbox = template_match_demo(scene1, ref_front, method)\\n\",\n    \"    \\n\",\n    \"    # Show result\\n\",\n    \"    axes[0, i].imshow(cv2.cvtColor(result_img, cv2.COLOR_BGR2RGB))\\n\",\n    \"    axes[0, i].set_title(f'{name}\\\\nConfidence: {confidence:.3f}')\\n\",\n    \"    axes[0, i].axis('off')\\n\",\n    \"    \\n\",\n    \"    # Show match heatmap\\n\",\n    \"    axes[1, i].imshow(match_result, cmap='hot')\\n\",\n    \"    axes[1, i].set_title(f'Match Heatmap: {name}')\\n\",\n    \"    axes[1, i].axis('off')\\n\",\n    \"\\n\",\n    \"plt.tight_layout()\\n\",\n    \"plt.show()\\n\",\n    \"\\n\",\n    \"print(\\\"üìä Template Matching Results:\\\")\\n\",\n    \"print(\\\"- TM_CCOEFF_NORMED: Best for most cases (normalized correlation)\\\")\\n\",\n    \"print(\\\"- TM_CCORR_NORMED: Good but can be fooled by bright areas\\\")\\n\",\n    \"print(\\\"- TM_SQDIFF_NORMED: Good but inverted (lower is better)\\\")\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"## 4. Exercise 2: Multi-Template Matching\\n\",\n    \"\\n\",\n    \"Now let's match against multiple reference angles:\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"def multi_template_match(scene, templates, template_names):\\n\",\n    \"    \\\"\\\"\\\"Match against multiple templates and return best result.\\\"\\\"\\\"\\n\",\n    \"    \\n\",\n    \"    best_confidence = 0\\n\",\n    \"    best_result = None\\n\",\n    \"    best_template = None\\n\",\n    \"    all_results = []\\n\",\n    \"    \\n\",\n    \"    for template, name in zip(templates, template_names):\\n\",\n    \"        result_img, match_result, confidence, bbox = template_match_demo(scene, template)\\n\",\n    \"        \\n\",\n    \"        all_results.append({\\n\",\n    \"            'name': name,\\n\",\n    \"            'confidence': confidence,\\n\",\n    \"            'result_img': result_img,\\n\",\n    \"            'bbox': bbox\\n\",\n    \"        })\\n\",\n    \"        \\n\",\n    \"        if confidence > best_confidence:\\n\",\n    \"            best_confidence = confidence\\n\",\n    \"            best_result = result_img\\n\",\n    \"            best_template = name\\n\",\n    \"    \\n\",\n    \"    return best_result, best_template, best_confidence, all_results\\n\",\n    \"\\n\",\n    \"# Test multi-template matching\\n\",\n    \"templates = [ref_front, ref_side, ref_back]\\n\",\n    \"template_names = ['Front', 'Side', 'Back']\\n\",\n    \"\\n\",\n    \"scenes = [scene1, scene2]\\n\",\n    \"scene_names = ['Scene 1', 'Scene 2']\\n\",\n    \"\\n\",\n    \"fig, axes = plt.subplots(2, 4, figsize=(20, 10))\\n\",\n    \"\\n\",\n    \"for scene_idx, (scene, scene_name) in enumerate(zip(scenes, scene_names)):\\n\",\n    \"    best_result, best_template, best_confidence, all_results = multi_template_match(\\n\",\n    \"        scene, templates, template_names\\n\",\n    \"    )\\n\",\n    \"    \\n\",\n    \"    # Show original scene\\n\",\n    \"    axes[scene_idx, 0].imshow(cv2.cvtColor(scene, cv2.COLOR_BGR2RGB))\\n\",\n    \"    axes[scene_idx, 0].set_title(f'{scene_name}\\\\n(Original)')\\n\",\n    \"    axes[scene_idx, 0].axis('off')\\n\",\n    \"    \\n\",\n    \"    # Show results for each template\\n\",\n    \"    for i, result in enumerate(all_results):\\n\",\n    \"        axes[scene_idx, i+1].imshow(cv2.cvtColor(result['result_img'], cv2.COLOR_BGR2RGB))\\n\",\n    \"        title = f\\\"{result['name']}\\\\nConf: {result['confidence']:.3f}\\\"\\n\",\n    \"        if result['name'] == best_template:\\n\",\n    \"            title += \\\" ‚≠ê\\\"\\n\",\n    \"        axes[scene_idx, i+1].set_title(title)\\n\",\n    \"        axes[scene_idx, i+1].axis('off')\\n\",\n    \"    \\n\",\n    \"    print(f\\\"{scene_name}: Best match is {best_template} with confidence {best_confidence:.3f}\\\")\\n\",\n    \"\\n\",\n    \"plt.tight_layout()\\n\",\n    \"plt.show()\\n\",\n    \"\\n\",\n    \"print(\\\"\\\\nüí° Observations:\\\")\\n\",\n    \"print(\\\"- Multiple templates improve detection reliability\\\")\\n\",\n    \"print(\\\"- Different views work better in different scenarios\\\")\\n\",\n    \"print(\\\"- Taking the best match across all templates is more robust\\\")\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"## 5. Exercise 3: ORB Feature Detection\\n\",\n    \"\\n\",\n    \"Now let's explore ORB (Oriented FAST and Rotated BRIEF) features:\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"def orb_feature_demo(image, max_features=500):\\n\",\n    \"    \\\"\\\"\\\"Demonstrate ORB feature detection.\\\"\\\"\\\"\\n\",\n    \"    \\n\",\n    \"    # Initialize ORB detector\\n\",\n    \"    orb = cv2.ORB_create(nfeatures=max_features)\\n\",\n    \"    \\n\",\n    \"    # Convert to grayscale\\n\",\n    \"    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\\n\",\n    \"    \\n\",\n    \"    # Detect keypoints and compute descriptors\\n\",\n    \"    keypoints, descriptors = orb.detectAndCompute(gray, None)\\n\",\n    \"    \\n\",\n    \"    # Draw keypoints\\n\",\n    \"    img_with_keypoints = cv2.drawKeypoints(image, keypoints, None, \\n\",\n    \"                                          flags=cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)\\n\",\n    \"    \\n\",\n    \"    return img_with_keypoints, keypoints, descriptors\\n\",\n    \"\\n\",\n    \"# Detect features in all images\\n\",\n    \"images = [ref_front, ref_side, ref_back, scene1, scene2]\\n\",\n    \"image_names = ['Ref Front', 'Ref Side', 'Ref Back', 'Scene 1', 'Scene 2']\\n\",\n    \"\\n\",\n    \"feature_results = []\\n\",\n    \"fig, axes = plt.subplots(2, 3, figsize=(18, 12))\\n\",\n    \"axes = axes.flatten()\\n\",\n    \"\\n\",\n    \"for i, (image, name) in enumerate(zip(images, image_names)):\\n\",\n    \"    img_with_features, keypoints, descriptors = orb_feature_demo(image)\\n\",\n    \"    feature_results.append((keypoints, descriptors))\\n\",\n    \"    \\n\",\n    \"    if i < len(axes):\\n\",\n    \"        axes[i].imshow(cv2.cvtColor(img_with_features, cv2.COLOR_BGR2RGB))\\n\",\n    \"        axes[i].set_title(f'{name}\\\\n{len(keypoints)} features')\\n\",\n    \"        axes[i].axis('off')\\n\",\n    \"\\n\",\n    \"# Hide the last subplot\\n\",\n    \"axes[-1].axis('off')\\n\",\n    \"\\n\",\n    \"plt.tight_layout()\\n\",\n    \"plt.show()\\n\",\n    \"\\n\",\n    \"print(\\\"üîç ORB Feature Detection Results:\\\")\\n\",\n    \"for i, name in enumerate(image_names):\\n\",\n    \"    keypoints, descriptors = feature_results[i]\\n\",\n    \"    desc_shape = descriptors.shape if descriptors is not None else 'None'\\n\",\n    \"    print(f\\\"  {name}: {len(keypoints)} keypoints, descriptors: {desc_shape}\\\")\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"## 6. Exercise 4: Feature Matching\\n\",\n    \"\\n\",\n    \"Let's match features between reference images and scenes:\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"def feature_matching_demo(ref_img, scene_img, ref_name, scene_name):\\n\",\n    \"    \\\"\\\"\\\"Demonstrate feature matching between reference and scene.\\\"\\\"\\\"\\n\",\n    \"    \\n\",\n    \"    # Initialize ORB\\n\",\n    \"    orb = cv2.ORB_create(nfeatures=1000)\\n\",\n    \"    \\n\",\n    \"    # Convert to grayscale\\n\",\n    \"    ref_gray = cv2.cvtColor(ref_img, cv2.COLOR_BGR2GRAY)\\n\",\n    \"    scene_gray = cv2.cvtColor(scene_img, cv2.COLOR_BGR2GRAY)\\n\",\n    \"    \\n\",\n    \"    # Detect features\\n\",\n    \"    ref_kp, ref_desc = orb.detectAndCompute(ref_gray, None)\\n\",\n    \"    scene_kp, scene_desc = orb.detectAndCompute(scene_gray, None)\\n\",\n    \"    \\n\",\n    \"    if ref_desc is None or scene_desc is None:\\n\",\n    \"        print(f\\\"No features found in {ref_name} or {scene_name}\\\")\\n\",\n    \"        return None, 0, 0\\n\",\n    \"    \\n\",\n    \"    # Create matcher\\n\",\n    \"    matcher = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck=True)\\n\",\n    \"    \\n\",\n    \"    # Match features\\n\",\n    \"    matches = matcher.match(ref_desc, scene_desc)\\n\",\n    \"    \\n\",\n    \"    # Sort matches by distance (best first)\\n\",\n    \"    matches = sorted(matches, key=lambda x: x.distance)\\n\",\n    \"    \\n\",\n    \"    # Filter good matches\\n\",\n    \"    good_matches = [m for m in matches if m.distance < 50]\\n\",\n    \"    \\n\",\n    \"    # Draw matches\\n\",\n    \"    match_img = cv2.drawMatches(ref_img, ref_kp, scene_img, scene_kp, \\n\",\n    \"                                good_matches[:20], None, \\n\",\n    \"                                flags=cv2.DrawMatchesFlags_NOT_DRAW_SINGLE_POINTS)\\n\",\n    \"    \\n\",\n    \"    # Calculate confidence\\n\",\n    \"    confidence = len(good_matches) / max(len(ref_kp), len(scene_kp))\\n\",\n    \"    confidence = min(1.0, confidence * 2)  # Scale up\\n\",\n    \"    \\n\",\n    \"    return match_img, len(good_matches), confidence\\n\",\n    \"\\n\",\n    \"# Test feature matching\\n\",\n    \"reference_images = [ref_front, ref_side, ref_back]\\n\",\n    \"reference_names = ['Front', 'Side', 'Back']\\n\",\n    \"test_scenes = [scene1, scene2]\\n\",\n    \"test_scene_names = ['Scene 1', 'Scene 2']\\n\",\n    \"\\n\",\n    \"for scene_idx, (scene, scene_name) in enumerate(zip(test_scenes, test_scene_names)):\\n\",\n    \"    print(f\\\"\\\\nüéØ Feature Matching Results for {scene_name}:\\\")\\n\",\n    \"    print(\\\"=\\\" * 50)\\n\",\n    \"    \\n\",\n    \"    fig, axes = plt.subplots(1, 3, figsize=(20, 6))\\n\",\n    \"    \\n\",\n    \"    best_confidence = 0\\n\",\n    \"    best_ref = None\\n\",\n    \"    \\n\",\n    \"    for ref_idx, (ref_img, ref_name) in enumerate(zip(reference_images, reference_names)):\\n\",\n    \"        match_img, num_matches, confidence = feature_matching_demo(\\n\",\n    \"            ref_img, scene, ref_name, scene_name\\n\",\n    \"        )\\n\",\n    \"        \\n\",\n    \"        if match_img is not None:\\n\",\n    \"            axes[ref_idx].imshow(cv2.cvtColor(match_img, cv2.COLOR_BGR2RGB))\\n\",\n    \"            title = f'{ref_name} vs {scene_name}\\\\n{num_matches} matches, conf: {confidence:.3f}'\\n\",\n    \"            axes[ref_idx].set_title(title)\\n\",\n    \"            axes[ref_idx].axis('off')\\n\",\n    \"            \\n\",\n    \"            print(f\\\"  {ref_name:8}: {num_matches:3d} matches, confidence: {confidence:.3f}\\\")\\n\",\n    \"            \\n\",\n    \"            if confidence > best_confidence:\\n\",\n    \"                best_confidence = confidence\\n\",\n    \"                best_ref = ref_name\\n\",\n    \"        else:\\n\",\n    \"            axes[ref_idx].text(0.5, 0.5, 'No matches found', \\n\",\n    \"                              ha='center', va='center', transform=axes[ref_idx].transAxes)\\n\",\n    \"            axes[ref_idx].set_title(f'{ref_name} vs {scene_name}\\\\nNo matches')\\n\",\n    \"            axes[ref_idx].axis('off')\\n\",\n    \"    \\n\",\n    \"    plt.tight_layout()\\n\",\n    \"    plt.show()\\n\",\n    \"    \\n\",\n    \"    if best_ref:\\n\",\n    \"        print(f\\\"  üèÜ Best match: {best_ref} (confidence: {best_confidence:.3f})\\\")\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"## 7. Exercise 5: Using the PlayerMatcher Class\\n\",\n    \"\\n\",\n    \"Now let's test our actual PlayerMatcher implementation:\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"# Create temporary reference directory\\n\",\n    \"import tempfile\\n\",\n    \"import shutil\\n\",\n    \"\\n\",\n    \"temp_dir = tempfile.mkdtemp()\\n\",\n    \"\\n\",\n    \"try:\\n\",\n    \"    # Save reference images\\n\",\n    \"    cv2.imwrite(os.path.join(temp_dir, 'front.jpg'), ref_front)\\n\",\n    \"    cv2.imwrite(os.path.join(temp_dir, 'side.jpg'), ref_side)\\n\",\n    \"    cv2.imwrite(os.path.join(temp_dir, 'back.jpg'), ref_back)\\n\",\n    \"    \\n\",\n    \"    # Test color-based matcher\\n\",\n    \"    print(\\\"üé® Testing Color-Based Player Matcher:\\\")\\n\",\n    \"    color_matcher = create_player_matcher('color', hex_color='#FF0000', tolerance=30)\\n\",\n    \"    \\n\",\n    \"    # Test reference-based matcher\\n\",\n    \"    print(\\\"\\\\nüîç Testing Reference-Based Player Matcher:\\\")\\n\",\n    \"    ref_matcher = create_player_matcher('reference', reference_dir=temp_dir)\\n\",\n    \"    \\n\",\n    \"    # Test both matchers on both scenes\\n\",\n    \"    matchers = [('Color', color_matcher), ('Reference', ref_matcher)]\\n\",\n    \"    scenes = [('Scene 1', scene1), ('Scene 2', scene2)]\\n\",\n    \"    \\n\",\n    \"    fig, axes = plt.subplots(2, 4, figsize=(20, 10))\\n\",\n    \"    \\n\",\n    \"    for scene_idx, (scene_name, scene) in enumerate(scenes):\\n\",\n    \"        for matcher_idx, (matcher_name, matcher) in enumerate(matchers):\\n\",\n    \"            # Find player\\n\",\n    \"            result = matcher.find_player_in_frame(scene)\\n\",\n    \"            \\n\",\n    \"            # Visualize result\\n\",\n    \"            vis_frame = matcher.visualize_detection(scene, result)\\n\",\n    \"            \\n\",\n    \"            # Display\\n\",\n    \"            col_idx = matcher_idx * 2 + (1 if result['found'] else 0)\\n\",\n    \"            if col_idx < 4:\\n\",\n    \"                axes[scene_idx, col_idx].imshow(cv2.cvtColor(vis_frame, cv2.COLOR_BGR2RGB))\\n\",\n    \"                \\n\",\n    \"                title = f\\\"{matcher_name} on {scene_name}\\\\n\\\"\\n\",\n    \"                if result['found']:\\n\",\n    \"                    title += f\\\"‚úÖ Found (conf: {result['confidence']:.3f})\\\"\\n\",\n    \"                else:\\n\",\n    \"                    title += \\\"‚ùå Not found\\\"\\n\",\n    \"                \\n\",\n    \"                axes[scene_idx, col_idx].set_title(title)\\n\",\n    \"                axes[scene_idx, col_idx].axis('off')\\n\",\n    \"            \\n\",\n    \"            # Print results\\n\",\n    \"            print(f\\\"  {matcher_name} on {scene_name}:\\\")\\n\",\n    \"            print(f\\\"    Found: {result['found']}\\\")\\n\",\n    \"            print(f\\\"    Confidence: {result['confidence']:.3f}\\\")\\n\",\n    \"            if result['bbox']:\\n\",\n    \"                print(f\\\"    Bounding box: {result['bbox']}\\\")\\n\",\n    \"            print()\\n\",\n    \"    \\n\",\n    \"    # Fill remaining subplots\\n\",\n    \"    for i in range(4):\\n\",\n    \"        for j in range(2):\\n\",\n    \"            if i >= 2:\\n\",\n    \"                axes[j, i].axis('off')\\n\",\n    \"    \\n\",\n    \"    plt.tight_layout()\\n\",\n    \"    plt.show()\\n\",\n    \"    \\n\",\n    \"finally:\\n\",\n    \"    # Clean up\\n\",\n    \"    shutil.rmtree(temp_dir)\\n\",\n    \"\\n\",\n    \"print(\\\"\\\\nüí° PlayerMatcher Observations:\\\")\\n\",\n    \"print(\\\"- Color matching works well when players have unique colors\\\")\\n\",\n    \"print(\\\"- Reference matching is more robust for complex scenarios\\\")\\n\",\n    \"print(\\\"- Both methods provide confidence scores for decision making\\\")\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"## 8. Performance Comparison\\n\",\n    \"\\n\",\n    \"Let's compare the performance characteristics:\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"import time\\n\",\n    \"\\n\",\n    \"def benchmark_matcher(matcher, scenes, num_runs=10):\\n\",\n    \"    \\\"\\\"\\\"Benchmark a player matcher.\\\"\\\"\\\"\\n\",\n    \"    times = []\\n\",\n    \"    results = []\\n\",\n    \"    \\n\",\n    \"    for scene in scenes:\\n\",\n    \"        scene_times = []\\n\",\n    \"        scene_results = []\\n\",\n    \"        \\n\",\n    \"        for _ in range(num_runs):\\n\",\n    \"            start_time = time.time()\\n\",\n    \"            result = matcher.find_player_in_frame(scene)\\n\",\n    \"            end_time = time.time()\\n\",\n    \"            \\n\",\n    \"            scene_times.append(end_time - start_time)\\n\",\n    \"            scene_results.append(result)\\n\",\n    \"        \\n\",\n    \"        times.append(scene_times)\\n\",\n    \"        results.append(scene_results)\\n\",\n    \"    \\n\",\n    \"    return times, results\\n\",\n    \"\\n\",\n    \"# Benchmark both matchers\\n\",\n    \"temp_dir = tempfile.mkdtemp()\\n\",\n    \"\\n\",\n    \"try:\\n\",\n    \"    # Save reference images\\n\",\n    \"    cv2.imwrite(os.path.join(temp_dir, 'front.jpg'), ref_front)\\n\",\n    \"    cv2.imwrite(os.path.join(temp_dir, 'side.jpg'), ref_side)\\n\",\n    \"    cv2.imwrite(os.path.join(temp_dir, 'back.jpg'), ref_back)\\n\",\n    \"    \\n\",\n    \"    # Create matchers\\n\",\n    \"    color_matcher = create_player_matcher('color', hex_color='#FF0000')\\n\",\n    \"    ref_matcher = create_player_matcher('reference', reference_dir=temp_dir)\\n\",\n    \"    \\n\",\n    \"    # Benchmark\\n\",\n    \"    print(\\\"‚è±Ô∏è Benchmarking Player Matchers...\\\")\\n\",\n    \"    \\n\",\n    \"    scenes = [scene1, scene2]\\n\",\n    \"    \\n\",\n    \"    color_times, color_results = benchmark_matcher(color_matcher, scenes, 5)\\n\",\n    \"    ref_times, ref_results = benchmark_matcher(ref_matcher, scenes, 5)\\n\",\n    \"    \\n\",\n    \"    # Analyze results\\n\",\n    \"    print(\\\"\\\\nüìä Performance Results:\\\")\\n\",\n    \"    print(\\\"=\\\" * 40)\\n\",\n    \"    \\n\",\n    \"    for scene_idx, scene_name in enumerate(['Scene 1', 'Scene 2']):\\n\",\n    \"        color_avg_time = np.mean(color_times[scene_idx]) * 1000  # ms\\n\",\n    \"        ref_avg_time = np.mean(ref_times[scene_idx]) * 1000  # ms\\n\",\n    \"        \\n\",\n    \"        color_found = color_results[scene_idx][0]['found']\\n\",\n    \"        ref_found = ref_results[scene_idx][0]['found']\\n\",\n    \"        \\n\",\n    \"        color_conf = color_results[scene_idx][0]['confidence']\\n\",\n    \"        ref_conf = ref_results[scene_idx][0]['confidence']\\n\",\n    \"        \\n\",\n    \"        print(f\\\"{scene_name}:\\\")\\n\",\n    \"        print(f\\\"  Color Method:     {color_avg_time:6.2f}ms | Found: {color_found} | Conf: {color_conf:.3f}\\\")\\n\",\n    \"        print(f\\\"  Reference Method: {ref_avg_time:6.2f}ms | Found: {ref_found} | Conf: {ref_conf:.3f}\\\")\\n\",\n    \"        print()\\n\",\n    \"    \\n\",\n    \"    # Plot performance comparison\\n\",\n    \"    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\\n\",\n    \"    \\n\",\n    \"    # Timing comparison\\n\",\n    \"    methods = ['Color', 'Reference']\\n\",\n    \"    scene1_times = [np.mean(color_times[0]) * 1000, np.mean(ref_times[0]) * 1000]\\n\",\n    \"    scene2_times = [np.mean(color_times[1]) * 1000, np.mean(ref_times[1]) * 1000]\\n\",\n    \"    \\n\",\n    \"    x = np.arange(len(methods))\\n\",\n    \"    width = 0.35\\n\",\n    \"    \\n\",\n    \"    ax1.bar(x - width/2, scene1_times, width, label='Scene 1', alpha=0.8)\\n\",\n    \"    ax1.bar(x + width/2, scene2_times, width, label='Scene 2', alpha=0.8)\\n\",\n    \"    ax1.set_xlabel('Method')\\n\",\n    \"    ax1.set_ylabel('Time (ms)')\\n\",\n    \"    ax1.set_title('Processing Time Comparison')\\n\",\n    \"    ax1.set_xticks(x)\\n\",\n    \"    ax1.set_xticklabels(methods)\\n\",\n    \"    ax1.legend()\\n\",\n    \"    ax1.grid(True, alpha=0.3)\\n\",\n    \"    \\n\",\n    \"    # Confidence comparison\\n\",\n    \"    color_confs = [color_results[0][0]['confidence'], color_results[1][0]['confidence']]\\n\",\n    \"    ref_confs = [ref_results[0][0]['confidence'], ref_results[1][0]['confidence']]\\n\",\n    \"    \\n\",\n    \"    ax2.bar(x - width/2, color_confs, width, label='Color', alpha=0.8)\\n\",\n    \"    ax2.bar(x + width/2, ref_confs, width, label='Reference', alpha=0.8)\\n\",\n    \"    ax2.set_xlabel('Scene')\\n\",\n    \"    ax2.set_ylabel('Confidence')\\n\",\n    \"    ax2.set_title('Detection Confidence Comparison')\\n\",\n    \"    ax2.set_xticks(x)\\n\",\n    \"    ax2.set_xticklabels(['Scene 1', 'Scene 2'])\\n\",\n    \"    ax2.legend()\\n\",\n    \"    ax2.grid(True, alpha=0.3)\\n\",\n    \"    ax2.set_ylim(0, 1)\\n\",\n    \"    \\n\",\n    \"    plt.tight_layout()\\n\",\n    \"    plt.show()\\n\",\n    \"    \\n\",\n    \"finally:\\n\",\n    \"    shutil.rmtree(temp_dir)\\n\",\n    \"\\n\",\n    \"print(\\\"\\\\nüéØ Key Takeaways:\\\")\\n\",\n    \"print(\\\"- Color matching is faster but less robust\\\")\\n\",\n    \"print(\\\"- Reference matching is slower but more accurate\\\")\\n\",\n    \"print(\\\"- Choose based on your use case and performance requirements\\\")\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"## 9. Real-World Considerations\\n\",\n    \"\\n\",\n    \"Let's discuss practical factors for production use:\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"def create_challenging_scene():\\n\",\n    \"    \\\"\\\"\\\"Create a more challenging test scenario.\\\"\\\"\\\"\\n\",\n    \"    \\n\",\n    \"    # Create scene with similar colors\\n\",\n    \"    challenging_scene = np.zeros((600, 800, 3), dtype=np.uint8)\\n\",\n    \"    challenging_scene[:, :] = [34, 139, 34]  # Green field\\n\",\n    \"    \\n\",\n    \"    # Add challenging elements\\n\",\n    \"    # Multiple red players (similar to target)\\n\",\n    \"    cv2.circle(challenging_scene, (200, 200), 30, (0, 0, 255), -1)  # Target\\n\",\n    \"    cv2.circle(challenging_scene, (400, 300), 25, (0, 0, 200), -1)  # Similar red\\n\",\n    \"    cv2.circle(challenging_scene, (600, 250), 28, (50, 0, 255), -1)  # Similar red\\n\",\n    \"    \\n\",\n    \"    # Add noise and shadows\\n\",\n    \"    cv2.ellipse(challenging_scene, (300, 400), (100, 50), 0, 0, 360, (20, 100, 20), -1)  # Shadow\\n\",\n    \"    \\n\",\n    \"    # Add motion blur simulation\\n\",\n    \"    kernel = np.ones((5, 15), np.float32) / 75\\n\",\n    \"    challenging_scene = cv2.filter2D(challenging_scene, -1, kernel)\\n\",\n    \"    \\n\",\n    \"    return challenging_scene\\n\",\n    \"\\n\",\n    \"challenging_scene = create_challenging_scene()\\n\",\n    \"\\n\",\n    \"plt.figure(figsize=(12, 8))\\n\",\n    \"plt.imshow(cv2.cvtColor(challenging_scene, cv2.COLOR_BGR2RGB))\\n\",\n    \"plt.title('Challenging Scene: Motion Blur + Similar Colors + Shadows')\\n\",\n    \"plt.axis('off')\\n\",\n    \"plt.show()\\n\",\n    \"\\n\",\n    \"print(\\\"üö® Real-World Challenges:\\\")\\n\",\n    \"print(\\\"\\\\n1. Lighting Conditions:\\\")\\n\",\n    \"print(\\\"   - Shadows can change apparent colors\\\")\\n\",\n    \"print(\\\"   - Different lighting (indoor vs outdoor)\\\")\\n\",\n    \"   - Camera auto-exposure changes\\\")\\n\",\n    \"\\n\",\n    \"print(\\\"2. Motion and Blur:\\\")\\n\",\n    \"print(\\\"   - Fast movement causes motion blur\\\")\\n\",\n    \"print(\\\"   - Features become less distinct\\\")\\n\",\n    \"print(\\\"   - Template matching fails with severe blur\\\")\\n\",\n    \"\\n\",\n    \"print(\\\"3. Scale and Perspective:\\\")\\n\",\n    \"print(\\\"   - Players appear different sizes based on distance\\\")\\n\",\n    \"print(\\\"   - Camera angle affects appearance\\\")\\n\",\n    \"print(\\\"   - Zoom level variations\\\")\\n\",\n    \"\\n\",\n    \"print(\\\"4. Occlusion:\\\")\\n\",\n    \"print(\\\"   - Players blocked by other players\\\")\\n\",\n    \"print(\\\"   - Partial visibility\\\")\\n\",\n    \"print(\\\"   - Equipment (balls, sticks) in front of players\\\")\\n\",\n    \"\\n\",\n    \"print(\\\"üí° Mitigation Strategies:\\\")\\n\",\n    \"print(\\\"\\n\",\n    \"print(\\\"1. Combine Multiple Methods:\\\")\\n\",\n    \"print(\\\"   - Use both color and feature detection\\\")\\n\",\n    \"print(\\\"   - Weight results based on confidence\\\")\\n\",\n    \"print(\\\"   - Fall back to alternative methods\\\")\\n\",\n    \"\\n\",\n    \"print(\\\"2. Temporal Consistency:\\\")\\n\",\n    \"print(\\\"   - Track player across multiple frames\\\")\\n\",\n    \"print(\\\"   - Use previous detections to guide current search\\\")\\n\",\n    \"print(\\\"   - Apply smoothing to reduce jitter\\\")\\n\",\n    \"\\n\",\n    \"print(\\\"3. Adaptive Thresholds:\\\")\\n\",\n    \"print(\\\"   - Adjust color tolerance based on lighting\\\")\\n\",\n    \"print(\\\"   - Lower confidence thresholds in challenging conditions\\\")\\n\",\n    \"print(\\\"   - Use multiple reference images\\\")\\n\",\n    \"\\n\",\n    \"print(\\\"4. Preprocessing:\\\")\\n\",\n    \"print(\\\"   - Normalize lighting/contrast\\\")\\n\",\n    \"print(\\\"   - Apply sharpening filters\\\")\\n\",\n    \"print(\\\"   - Use histogram equalization\\\")\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"## 10. Final Exercise: Build Your Own Matcher\\n\",\n    \"\\n\",\n    \"Now it's your turn! Implement a hybrid matcher that combines the best of both approaches:\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"class HybridPlayerMatcher:\\n\",\n    \"    \\\"\\\"\\\"A hybrid matcher that combines color and feature detection.\\\"\\\"\\\"\\n\",\n    \"    \\n\",\n    \"    def __init__(self, hex_color, reference_dir, color_weight=0.4, feature_weight=0.6):\\n\",\n    \"        self.color_matcher = create_player_matcher('color', hex_color=hex_color)\\n\",\n    \"        self.feature_matcher = create_player_matcher('reference', reference_dir=reference_dir)\\n\",\n    \"        self.color_weight = color_weight\\n\",\n    \"        self.feature_weight = feature_weight\\n\",\n    \"    \\n\",\n    \"    def find_player_in_frame(self, frame):\\n\",\n    \"        \\\"\\\"\\\"Find player using hybrid approach.\\\"\\\"\\\"\\n\",\n    \"        \\n\",\n    \"        # Get results from both methods\\n\",\n    \"        color_result = self.color_matcher.find_player_in_frame(frame)\\n\",\n    \"        feature_result = self.feature_matcher.find_player_in_frame(frame)\\n\",\n    \"        \\n\",\n    \"        # Calculate weighted confidence\\n\",\n    \"        color_conf = color_result['confidence'] if color_result['found'] else 0\\n\",\n    \"        feature_conf = feature_result['confidence'] if feature_result['found'] else 0\\n\",\n    \"        \\n\",\n    \"        combined_confidence = (\\n\",\n    \"            self.color_weight * color_conf + \\n\",\n    \"            self.feature_weight * feature_conf\\n\",\n    \"        )\\n\",\n    \"        \\n\",\n    \"        # Choose the best result\\n\",\n    \"        if feature_conf > color_conf:\\n\",\n    \"            best_result = feature_result\\n\",\n    \"            best_method = 'feature'\\n\",\n    \"        else:\\n\",\n    \"            best_result = color_result\\n\",\n    \"            best_method = 'color'\\n\",\n    \"        \\n\",\n    \"        # Create hybrid result\\n\",\n    \"        hybrid_result = {\\n\",\n    \"            'found': combined_confidence > 0.3,  # Combined threshold\\n\",\n    \"            'confidence': combined_confidence,\\n\",\n    \"            'bbox': best_result['bbox'],\\n\",\n    \"            'method_data': {\\n\",\n    \"                'best_method': best_method,\\n\",\n    \"                'color_confidence': color_conf,\\n\",\n    \"                'feature_confidence': feature_conf,\\n\",\n    \"                'color_result': color_result,\\n\",\n    \"                'feature_result': feature_result\\n\",\n    \"            }\\n\",\n    \"        }\\n\",\n    \"        \\n\",\n    \"        return hybrid_result\\n\",\n    \"    \\n\",\n    \"    def visualize_detection(self, frame, result):\\n\",\n    \"        \\\"\\\"\\\"Visualize hybrid detection results.\\\"\\\"\\\"\\n\",\n    \"        \\n\",\n    \"        vis_frame = frame.copy()\\n\",\n    \"        \\n\",\n    \"        if not result['found']:\\n\",\n    \"            cv2.putText(vis_frame, 'Player not found', (10, 30),\\n\",\n    \"                       cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\\n\",\n    \"            return vis_frame\\n\",\n    \"        \\n\",\n    \"        # Draw bounding box\\n\",\n    \"        if result['bbox']:\\n\",\n    \"            x, y, w, h = result['bbox']\\n\",\n    \"            cv2.rectangle(vis_frame, (x, y), (x + w, y + h), (0, 255, 0), 3)\\n\",\n    \"            \\n\",\n    \"            # Show hybrid confidence\\n\",\n    \"            conf_text = f\\\"Hybrid: {result['confidence']:.3f}\\\"\\n\",\n    \"            cv2.putText(vis_frame, conf_text, (x, y - 40),\\n\",\n    \"                       cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)\\n\",\n    \"            \\n\",\n    \"            # Show method details\\n\",\n    \"            method_data = result['method_data']\\n\",\n    \"            method_text = f\\\"Best: {method_data['best_method']}\\\"\\n\",\n    \"            cv2.putText(vis_frame, method_text, (x, y - 15),\\n\",\n    \"                       cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 0), 2)\\n\",\n    \"            \\n\",\n    \"            # Show individual confidences\\n\",\n    \"            color_text = f\\\"C: {method_data['color_confidence']:.2f}\\\"\\n\",\n    \"            feature_text = f\\\"F: {method_data['feature_confidence']:.2f}\\\"\\n\",\n    \"            cv2.putText(vis_frame, color_text, (10, 60),\\n\",\n    \"                       cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 0, 0), 2)\\n\",\n    \"            cv2.putText(vis_frame, feature_text, (10, 85),\\n\",\n    \"                       cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 255), 2)\\n\",\n    \"        \\n\",\n    \"        return vis_frame\\n\",\n    \"\\n\",\n    \"# Test the hybrid matcher\\n\",\n    \"temp_dir = tempfile.mkdtemp()\\n\",\n    \"\\n\",\n    \"try:\\n\",\n    \"    # Save reference images\\n\",\n    \"    cv2.imwrite(os.path.join(temp_dir, 'front.jpg'), ref_front)\\n\",\n    \"    cv2.imwrite(os.path.join(temp_dir, 'side.jpg'), ref_side)\\n\",\n    \"    cv2.imwrite(os.path.join(temp_dir, 'back.jpg'), ref_back)\\n\",\n    \"    \\n\",\n    \"    # Create hybrid matcher\\n\",\n    \"    hybrid_matcher = HybridPlayerMatcher('#FF0000', temp_dir)\\n\",\n    \"    \\n\",\n    \"    # Test on all scenes\\n\",\n    \"    test_scenes = [scene1, scene2, challenging_scene]\\n\",\n    \"    scene_names = ['Scene 1', 'Scene 2', 'Challenging']\\n\",\n    \"    \\n\",\n    \"    fig, axes = plt.subplots(1, 3, figsize=(18, 6))\\n\",\n    \"    \\n\",\n    \"    print(\\\"ü§ñ Hybrid Matcher Results:\\\")\\n\",\n    \"    print(\\\"=\\\" * 40)\\n\",\n    \"    \\n\",\n    \"    for i, (scene, name) in enumerate(zip(test_scenes, scene_names)):\\n\",\n    \"        result = hybrid_matcher.find_player_in_frame(scene)\\n\",\n    \"        vis_frame = hybrid_matcher.visualize_detection(scene, result)\\n\",\n    \"        \\n\",\n    \"        axes[i].imshow(cv2.cvtColor(vis_frame, cv2.COLOR_BGR2RGB))\\n\",\n    \"        axes[i].set_title(f'{name}\\\\nHybrid Result')\\n\",\n    \"        axes[i].axis('off')\\n\",\n    \"        \\n\",\n    \"        # Print detailed results\\n\",\n    \"        method_data = result['method_data']\\n\",\n    \"        print(f\\\"{name}:\\\")\\n\",\n    \"        print(f\\\"  Found: {result['found']}\\\")\\n\",\n    \"        print(f\\\"  Combined confidence: {result['confidence']:.3f}\\\")\\n\",\n    \"        print(f\\\"  Best method: {method_data['best_method']}\\\")\\n\",\n    \"        print(f\\\"  Color confidence: {method_data['color_confidence']:.3f}\\\")\\n\",\n    \"        print(f\\\"  Feature confidence: {method_data['feature_confidence']:.3f}\\\")\\n\",\n    \"        print()\\n\",\n    \"    \\n\",\n    \"    plt.tight_layout()\\n\",\n    \"    plt.show()\\n\",\n    \"    \\n\",\n    \"finally:\\n\",\n    \"    shutil.rmtree(temp_dir)\\n\",\n    \"\\n\",\n    \"print(\\\"üéâ Congratulations! You've implemented a hybrid player matcher!\\\")\\n\",\n    \"print(\\\"\\\\nüöÄ Next Steps:\\\")\\n\",\n    \"print(\\\"- Experiment with different weight combinations\\\")\\n\",\n    \"print(\\\"- Add temporal tracking across video frames\\\")\\n\",\n    \"print(\\\"- Implement adaptive thresholds based on scene conditions\\\")\\n\",\n    \"print(\\\"- Try different feature detectors (SIFT, SURF, AKAZE)\\\")\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"## Conclusion\\n\",\n    \"\\n\",\n    \"üéØ **What you've learned:**\\n\",\n    \"\\n\",\n    \"1. ‚úÖ **Template Matching** - How to find objects using reference images\\n\",\n    \"2. ‚úÖ **ORB Feature Detection** - Advanced feature-based matching robust to rotation and scale\\n\",\n    \"3. ‚úÖ **Multi-Angle Matching** - Using multiple reference views for better coverage\\n\",\n    \"4. ‚úÖ **Performance Analysis** - Understanding trade-offs between speed and accuracy\\n\",\n    \"5. ‚úÖ **Real-World Challenges** - Dealing with lighting, blur, occlusion, and similar objects\\n\",\n    \"6. ‚úÖ **Hybrid Approaches** - Combining multiple methods for robust detection\\n\",\n    \"\\n\",\n    \"üîß **Practical Implementation Tips:**\\n\",\n    \"\\n\",\n    \"- **Color detection** is fast and works well for unique colors\\n\",\n    \"- **Feature matching** is slower but more robust to variations\\n\",\n    \"- **Multiple reference images** significantly improve detection rates\\n\",\n    \"- **Hybrid approaches** combine the strengths of both methods\\n\",\n    \"- **Confidence scores** help make intelligent decisions\\n\",\n    \"\\n\",\n    \"üéÆ **Try These Extensions:**\\n\",\n    \"\\n\",\n    \"1. **Different Feature Detectors**: Try SIFT, SURF, or AKAZE instead of ORB\\n\",\n    \"2. **Machine Learning**: Use a CNN to learn player appearance\\n\",\n    \"3. **Tracking**: Implement Kalman filters for temporal consistency\\n\",\n    \"4. **Multi-Scale**: Handle players at different distances\\n\",\n    \"5. **Real Video**: Test with actual sports footage\\n\",\n    \"\\n\",\n    \"**You're now ready to implement robust player detection in Action Shot Extractor! üöÄ**\"\n   ]\n  }\n ],\n \"metadata\": {\n  \"kernelspec\": {\n   \"display_name\": \"Python 3\",\n   \"language\": \"python\",\n   \"name\": \"python3\"\n  },\n  \"language_info\": {\n   \"codemirror_mode\": {\n    \"name\": \"ipython\",\n    \"version\": 3\n   },\n   \"file_extension\": \".py\",\n   \"mimetype\": \"text/x-python\",\n   \"name\": \"python\",\n   \"nbconvert_exporter\": \"python\",\n   \"pygments_lexer\": \"ipython3\",\n   \"version\": \"3.13.0\"\n  }\n },\n \"nbformat\": 4,\n \"nbformat_minor\": 4\n}